{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMq3DnUdqIX3",
        "outputId": "4d1755aa-7a01-4ac0-f2f5-8f8803cac6c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NLTK Data Path: ['/root/nltk_data', '/usr/nltk_data', '/usr/share/nltk_data', '/usr/lib/nltk_data', '/usr/share/nltk_data', '/usr/local/share/nltk_data', '/usr/lib/nltk_data', '/usr/local/lib/nltk_data', '/content/nltk_data']\n",
            "\n",
            "Memastikan data NLTK (punkt) ada di /content/nltk_data...\n",
            "‚úÖ Data NLTK (punkt) sudah tersedia di lokasi kustom.\n",
            "üí° Menggunakan Perangkat: Tesla T4\n",
            "\n",
            "Menghubungkan Google Drive...\n",
            "Mounted at /content/drive\n",
            "‚úÖ Google Drive terhubung.\n",
            "‚úÖ Folder dataset ditemukan di '/content/drive/MyDrive/paper_TA'.\n"
          ]
        }
      ],
      "source": [
        "import pdfplumber\n",
        "import docx\n",
        "import os\n",
        "import time\n",
        "import json # For saving results\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "from keybert import KeyBERT\n",
        "from transformers import pipeline\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import torch # To check for GPU availability\n",
        "import nltk\n",
        "import re # <-- TAMBAHAN: Untuk Regular Expressions (mencari referensi)\n",
        "\n",
        "# --- Global Settings & Configuration ---\n",
        "# [cite_start]Path ke folder dataset Anda di Google Drive [cite: 323, 330]\n",
        "DATASET_FOLDER = \"/content/drive/MyDrive/paper_TA\"\n",
        "# Path untuk menyimpan hasil JSON\n",
        "RESULTS_JSON_PATH = \"/content/drive/MyDrive/PaperTA_results.json\"\n",
        "GRAPH_JSON_PATH = \"/content/drive/MyDrive/PaperTA_graph_data.json\"\n",
        "SENTENCE_RELATIONS_JSON_PATH = \"/content/drive/MyDrive/PaperTA_sentence_relations.json\"\n",
        "\n",
        "# Batas karakter untuk input model (pencegahan error GPU/memori)\n",
        "# Sesuaikan jika perlu, 4000 char ~ 1000-1200 tokens\n",
        "MAX_CHARS_FOR_MODEL = 3000\n",
        "# Ambang batas kemiripan untuk visualisasi hubungan\n",
        "SIMILARITY_THRESHOLD = 0.50\n",
        "SENTENCE_SIMILARITY_THRESHOLD = 0.50\n",
        "\n",
        "NLTK_DATA_PATH = \"/content/nltk_data\"\n",
        "os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
        "\n",
        "if NLTK_DATA_PATH not in nltk.data.path:\n",
        "    nltk.data.path.append(NLTK_DATA_PATH)\n",
        "print(f\"NLTK Data Path: {nltk.data.path}\")\n",
        "\n",
        "print(f\"\\nMemastikan data NLTK (punkt) ada di {NLTK_DATA_PATH}...\")\n",
        "try:\n",
        "    # Coba cari di path spesifik kita\n",
        "    nltk.data.find('tokenizers/punkt', paths=[NLTK_DATA_PATH])\n",
        "    print(\"‚úÖ Data NLTK (punkt) sudah tersedia di lokasi kustom.\")\n",
        "except LookupError:\n",
        "    print(f\"‚ö† Data NLTK (punkt) tidak ditemukan, mencoba mengunduh ke {NLTK_DATA_PATH}...\")\n",
        "    try:\n",
        "        nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=True) # Download ke folder spesifik\n",
        "        # Verifikasi ulang setelah download\n",
        "        nltk.data.find('tokenizers/punkt', paths=[NLTK_DATA_PATH])\n",
        "        print(\"‚úÖ Data NLTK (punkt) berhasil diunduh dan diverifikasi.\")\n",
        "    except Exception as download_error:\n",
        "        print(f\"‚ùå FATAL ERROR: Gagal mengunduh data NLTK (punkt). Error: {download_error}\")\n",
        "        raise download_error # Hentikan jika download gagal\n",
        "\n",
        "# Cek ketersediaan GPU\n",
        "DEVICE = 0 if torch.cuda.is_available() else -1 # 0 for first GPU, -1 for CPU\n",
        "DEVICE_NAME = torch.cuda.get_device_name(0) if DEVICE == 0 else \"CPU\"\n",
        "print(f\"üí° Menggunakan Perangkat: {DEVICE_NAME}\")\n",
        "\n",
        "# --- Mount Google Drive ---\n",
        "print(\"\\nMenghubungkan Google Drive...\")\n",
        "try:\n",
        "    drive.mount('/content/drive', force_remount=True) # force_remount helps avoid stale mounts\n",
        "    print(\"‚úÖ Google Drive terhubung.\")\n",
        "    # Verifikasi path dataset\n",
        "    if not os.path.exists(DATASET_FOLDER):\n",
        "        print(f\"‚ö† Peringatan: Folder dataset '{DATASET_FOLDER}' tidak ditemukan di Google Drive Anda.\")\n",
        "    else:\n",
        "        print(f\"‚úÖ Folder dataset ditemukan di '{DATASET_FOLDER}'.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Gagal menghubungkan Google Drive: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hc8ylTJtqIDW",
        "outputId": "07a46f5c-942b-4a9c-a946-4c8ef13853e8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Fungsi helper (dengan download punkt paksa di split_into_sentences dan extract_references) siap.\n"
          ]
        }
      ],
      "source": [
        "import nltk # Pastikan diimpor\n",
        "import os # Pastikan diimpor\n",
        "import pdfplumber # Pastikan diimpor\n",
        "import docx # Pastikan diimpor\n",
        "import re # Pastikan diimpor\n",
        "\n",
        "NLTK_DATA_PATH = \"/content/nltk_data\"\n",
        "os.makedirs(NLTK_DATA_PATH, exist_ok=True)\n",
        "\n",
        "def split_into_sentences(text):\n",
        "    \"\"\"Memisahkan teks panjang menjadi kalimat menggunakan NLTK,\n",
        "       dengan MEMAKSA download/verifikasi punkt SETIAP KALI.\"\"\"\n",
        "    if not text or not isinstance(text, str):\n",
        "        return []\n",
        "\n",
        "    # --- LANGKAH KRUSIAL: PASTIKAN PUNKT DIUNDUH/ADA DI SINI ---\n",
        "    try:\n",
        "        #print(\"DEBUG: Memaksa download punkt...\") # Uncomment jika perlu debug\n",
        "        nltk.download('punkt', download_dir=NLTK_DATA_PATH, quiet=True, force=True, raise_on_error=True)\n",
        "        # Pastikan path dikenali (meskipun seharusnya sudah)\n",
        "        if NLTK_DATA_PATH not in nltk.data.path:\n",
        "            nltk.data.path.append(NLTK_DATA_PATH)\n",
        "        # print(\"DEBUG: Download selesai, path:\", nltk.data.path) # Uncomment jika perlu debug\n",
        "    except Exception as download_error:\n",
        "        print(f\"‚ùå FATAL ERROR: Gagal download paksa punkt. Error: {download_error}\")\n",
        "        return [] # Kembalikan list kosong\n",
        "    # -----------------------------------------------\n",
        "\n",
        "    # --- Panggil sent_tokenize ---\n",
        "    try:\n",
        "        # Panggil setelah download paksa\n",
        "        sentences = nltk.sent_tokenize(text)\n",
        "        return [s.strip() for s in sentences if len(s.strip()) > 10]\n",
        "    except LookupError as le:\n",
        "         # Jika MASIH LookupError, ini sangat aneh\n",
        "         print(f\"‚ùå Error LookupError saat sent_tokenize MESKIPUN sudah download paksa: {le}\")\n",
        "         print(f\"   NLTK mencari di: {nltk.data.path}\")\n",
        "         # Coba verifikasi manual file yang dicari\n",
        "         try:\n",
        "             finder = nltk.data.find('tokenizers/punkt', paths=[NLTK_DATA_PATH])\n",
        "             print(f\"   VERIFIKASI MANUAL: Ditemukan di {finder}\")\n",
        "         except LookupError:\n",
        "             print(f\"   VERIFIKASI MANUAL: Tetap tidak ditemukan di {NLTK_DATA_PATH}.\")\n",
        "         return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error lain saat memanggil nltk.sent_tokenize: {e}\")\n",
        "        return []\n",
        "\n",
        "def extract_text_from_pdf(file_path):\n",
        "    \"\"\"Mengekstrak teks mentah dari file PDF, dengan pembersihan dasar.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        with pdfplumber.open(file_path) as pdf:\n",
        "            for page in pdf.pages:\n",
        "                # x_tolerance=1 mengurangi spasi antar kata yang salah gabung\n",
        "                # y_tolerance=3 membantu menggabungkan paragraf yang terpisah dekat\n",
        "                page_text = page.extract_text(x_tolerance=1, y_tolerance=3)\n",
        "                if page_text:\n",
        "                    text += page_text + \"\\n\"\n",
        "        # Pembersihan: hapus baris baru berlebih & spasi di awal/akhir baris\n",
        "        cleaned_lines = [line.strip() for line in text.splitlines() if line.strip()]\n",
        "        return \"\\n\".join(cleaned_lines)\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Gagal memproses PDF {os.path.basename(file_path)}: {e}\")\n",
        "        return None\n",
        "\n",
        "def extract_text_from_docx(file_path):\n",
        "    \"\"\"Mengekstrak teks mentah dari file DOCX, dengan pembersihan dasar.\"\"\"\n",
        "    text = \"\"\n",
        "    try:\n",
        "        doc = docx.Document(file_path)\n",
        "        paragraphs = [para.text.strip() for para in doc.paragraphs if para.text.strip()]\n",
        "        return \"\\n\\n\".join(paragraphs) # Beri jarak antar paragraf\n",
        "    except Exception as e:\n",
        "        print(f\"‚ö† Gagal memproses DOCX {os.path.basename(file_path)}: {e}\")\n",
        "        return None\n",
        "\n",
        "# --- FUNGSI BARU UNTUK EKSTRAKSI REFERENSI ---\n",
        "def extract_references(full_text):\n",
        "    \"\"\"\n",
        "    Mencoba mengekstrak daftar referensi dari teks lengkap menggunakan regex.\n",
        "    \"\"\"\n",
        "    if not full_text:\n",
        "        return []\n",
        "\n",
        "    # Pola regex untuk menemukan awal bagian referensi\n",
        "    # Mencari di awal baris (^) dengan spasi opsional (\\s*)\n",
        "    # \\b memastikan kita mencocokkan kata utuh (misal \"References\" bukan \"Sub-references\")\n",
        "    pattern = re.compile(r'^\\s*(\\bREFERENCES\\b|\\bDAFTAR PUSTAKA\\b|\\bBIBLIOGRAPHY\\b)\\s*$',\n",
        "                         re.IGNORECASE | re.MULTILINE)\n",
        "\n",
        "    match = pattern.search(full_text)\n",
        "\n",
        "    if not match:\n",
        "        # Coba cari tanpa anchor akhir ($) jika judulnya ada di baris yang sama dengan teks lain\n",
        "        pattern_fallback = re.compile(r'^\\s*(\\bREFERENCES\\b|\\bDAFTAR PUSTAKA\\b|\\bBIBLIOGRAPHY\\b)',\n",
        "                                      re.IGNORECASE | re.MULTILINE)\n",
        "        match = pattern_fallback.search(full_text)\n",
        "\n",
        "    if not match:\n",
        "        print(\"‚Ñπ 'References'/'Daftar Pustaka' section not found.\")\n",
        "        return []\n",
        "\n",
        "    # Ambil semua teks SETELAH judul \"REFERENCES\"\n",
        "    text_after = full_text[match.end():].strip()\n",
        "\n",
        "    if not text_after:\n",
        "         print(\"‚Ñπ 'References' section found but was empty.\")\n",
        "         return []\n",
        "\n",
        "    # Heuristik untuk memisahkan entri referensi:\n",
        "    # 1. Pisahkan teks referensi menjadi baris-baris\n",
        "    # 2. Asumsikan entri BARU jika baris dimulai dengan:\n",
        "    #    - [angka] (misal [1], [23])\n",
        "    #    - Author, (misal Ahmed, U.)\n",
        "    #    - angka. (misal 1., 23.)\n",
        "    # 3. Jika tidak, anggap itu adalah kelanjutan dari entri sebelumnya.\n",
        "\n",
        "    lines = text_after.splitlines()\n",
        "    references = []\n",
        "    current_ref = \"\"\n",
        "\n",
        "    for line in lines:\n",
        "        stripped_line = line.strip()\n",
        "        if not stripped_line: # Lewati baris kosong\n",
        "            continue\n",
        "\n",
        "        is_new_entry = False\n",
        "        # Cek apakah ini awal dari entri baru\n",
        "        if re.match(r'^\\s*\\[\\d+\\]', stripped_line): # Pola [1]\n",
        "            is_new_entry = True\n",
        "        elif re.match(r'^\\s*[A-Z][a-z]+,', stripped_line): # Pola Author, (heuristik)\n",
        "            is_new_entry = True\n",
        "        elif re.match(r'^\\s*\\d+\\.', stripped_line): # Pola 1.\n",
        "             is_new_entry = True\n",
        "\n",
        "        if is_new_entry:\n",
        "            if current_ref: # Simpan referensi sebelumnya\n",
        "                references.append(current_ref.strip())\n",
        "            current_ref = stripped_line # Mulai referensi baru\n",
        "        else:\n",
        "            # Ini adalah kelanjutan dari referensi sebelumnya\n",
        "            if current_ref: # Hanya tambahkan jika sudah ada current_ref\n",
        "                current_ref += \" \" + stripped_line\n",
        "\n",
        "    if current_ref: # Simpan referensi terakhir\n",
        "        references.append(current_ref.strip())\n",
        "\n",
        "    # Filter hasil akhir (jika ada yang terlalu pendek/aneh)\n",
        "    final_references = [ref for ref in references if len(ref) > 20] # Filter referensi yg terlalu pendek\n",
        "\n",
        "    if not final_references and text_after:\n",
        "        # Fallback: Jika heuristik gagal tapi teks ada, kembalikan teks sebagai satu blok\n",
        "        print(\"‚Ñπ Heuristik pemisahan referensi gagal, mengembalikan sebagai blok teks.\")\n",
        "        return [text_after]\n",
        "\n",
        "    return final_references\n",
        "# --- AKHIR FUNGSI BARU ---\n",
        "\n",
        "\n",
        "def extract_keywords_bert(text, kw_model_instance, top_n=10, ngram_range=(1, 3)):\n",
        "    \"\"\"Mengekstrak kata kunci, dengan memotong teks dan error handling lebih baik.\"\"\"\n",
        "    if not text or not isinstance(text, str) or len(text.strip()) < 50:\n",
        "        return []\n",
        "    try:\n",
        "        truncated_text = text[:MAX_CHARS_FOR_MODEL]\n",
        "        keywords = kw_model_instance.extract_keywords(truncated_text,\n",
        "                                                 keyphrase_ngram_range=ngram_range,\n",
        "                                                 stop_words='english',\n",
        "                                                 use_maxsum=True, nr_candidates=20, top_n=top_n)\n",
        "        filtered_keywords = [(kw, score) for kw, score in keywords if len(kw) > 2 and not kw.isdigit()]\n",
        "        return [kw for kw, score in filtered_keywords]\n",
        "    # Tangkap RuntimeError secara spesifik untuk masalah CUDA\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA\" in str(e):\n",
        "            print(f\"‚ùå Error CUDA (Runtime) saat ekstraksi KeyBERT.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error Runtime saat ekstraksi KeyBERT: {e}\")\n",
        "        return []\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error Lain saat ekstraksi KeyBERT: {e}\")\n",
        "        return []\n",
        "\n",
        "def generate_summary_bart(text, summarizer_instance):\n",
        "    \"\"\"Menghasilkan ringkasan, dengan memotong teks dan error handling lebih baik.\"\"\"\n",
        "    if not text or not isinstance(text, str) or len(text.strip()) < 100:\n",
        "        return \"Teks terlalu pendek untuk diringkas.\"\n",
        "    try:\n",
        "        truncated_text = text[:MAX_CHARS_FOR_MODEL]\n",
        "        summary = summarizer_instance(truncated_text, max_length=150, min_length=30, do_sample=False)\n",
        "        return summary[0]['summary_text']\n",
        "    # Tangkap RuntimeError secara spesifik untuk masalah CUDA\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA\" in str(e):\n",
        "            print(f\"‚ùå Error CUDA (Runtime) saat peringkasan BART.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error Runtime saat peringkasan BART: {e}\")\n",
        "        return \"Gagal membuat ringkasan (Runtime Error).\"\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error Lain saat peringkasan BART: {e}\")\n",
        "        return \"Gagal membuat ringkasan (Error Lain).\"\n",
        "\n",
        "def generate_embeddings(text, embedding_model_instance):\n",
        "    \"\"\"Menghasilkan embedding, dengan memotong teks dan error handling lebih baik.\"\"\"\n",
        "    if not text or not isinstance(text, str) or len(text.strip()) < 50:\n",
        "        return None\n",
        "    try:\n",
        "        # Kita potong lebih panjang untuk embedding, tapi tetap berisiko\n",
        "        truncated_text = text[:MAX_CHARS_FOR_MODEL * 2] # Batas 6000 char\n",
        "        embedding = embedding_model_instance.encode(truncated_text, convert_to_numpy=True)\n",
        "        return embedding\n",
        "    # Tangkap RuntimeError secara spesifik untuk masalah CUDA\n",
        "    except RuntimeError as e:\n",
        "        if \"CUDA\" in str(e):\n",
        "            print(f\"‚ùå Error CUDA (Runtime) saat membuat embedding.\")\n",
        "        else:\n",
        "            print(f\"‚ùå Error Runtime saat membuat embedding: {e}\")\n",
        "        return None\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error Lain saat membuat embedding: {e}\")\n",
        "        return None\n",
        "\n",
        "print(\"‚úÖ Fungsi helper (dengan download punkt paksa di split_into_sentences dan extract_references) siap.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EGkMatjlqQ3o",
        "outputId": "fef7fc59-b47d-408e-d2ef-8dcf26e5a2ff"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚è≥ Memuat model KeyBERT (all-MiniLM-L6-v2)...\n",
            "‚úÖ Model KeyBERT berhasil dimuat.\n",
            "‚è≥ Memuat model Peringkasan (BART - distilbart-cnn-12-6)...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚úÖ Model Peringkasan berhasil dimuat.\n",
            "‚è≥ Memuat model Embedding (all-MiniLM-L6-v2)...\n",
            "‚úÖ Model Embedding berhasil dimuat.\n"
          ]
        }
      ],
      "source": [
        "print(\"‚è≥ Memuat model KeyBERT (all-MiniLM-L6-v2)...\")\n",
        "# Menggunakan model standar yang bagus untuk semantic tasks\n",
        "kw_model = KeyBERT(model='all-MiniLM-L6-v2')\n",
        "print(\"‚úÖ Model KeyBERT berhasil dimuat.\")\n",
        "\n",
        "print(\"‚è≥ Memuat model Peringkasan (BART - distilbart-cnn-12-6)...\")\n",
        "# Model ini bagus untuk ringkasan berita/artikel umum, cukup ringan\n",
        "summarizer = pipeline(\"summarization\", model=\"sshleifer/distilbart-cnn-12-6\", device=DEVICE)\n",
        "print(\"‚úÖ Model Peringkasan berhasil dimuat.\")\n",
        "\n",
        "print(\"‚è≥ Memuat model Embedding (all-MiniLM-L6-v2)...\")\n",
        "# Menggunakan model yang sama dengan KeyBERT untuk efisiensi & konsistensi embedding\n",
        "embedding_model = SentenceTransformer('all-MiniLM-L6-v2', device='cuda' if DEVICE == 0 else 'cpu')\n",
        "print(\"‚úÖ Model Embedding berhasil dimuat.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "POc8WfixqX5p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1a0244-61e2-47ff-9c5a-b6e4945ed05e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Memulai Proses Inti dari folder: /content/drive/MyDrive/paper_TA\n",
            "==================================================\n",
            "üîç Menemukan 11 file PDF/DOCX untuk diproses.\n",
            "\n",
            "[1/11]--- Processing: Using Financial News Sentiment for Stock Price Direction Prediction.pdf ---\n",
            "üìÑ Teks diekstrak (60,129 chars) dalam 2.62 dtk.\n",
            "üîë Kata Kunci (10): ['sentiment information analysis', 'prediction keywords sentiment', 'sentiment scores titles', 'predicted sentiment scores', 'sentiment information texts', 'sentiment classification', 'financial market sentiment', 'news sentiment stock', 'extract market sentiment', 'financial news sentiment'] (5.12 dtk)\n",
            "üìù Ringkasan:  Using sentiment information in the analysis of financial markets has attracted much attention . Natural language processing methods can be used to ex... (0.74 dtk)\n",
            "üìö Referensi diekstrak (30 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: 1. Nassirtoussi, A.K.; Aghabozorgi, S.; Wah, T.Y.; Ngo, D.C.L. Text mining for market prediction: A ...\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "[2/11]--- Processing: Deep Learning for Aspect-Based Sentiment Analysis A Comparative Review.pdf ---\n",
            "üìÑ Teks diekstrak (141,266 chars) dalam 15.36 dtk.\n",
            "üîë Kata Kunci (10): ['content web sentiment', 'reviews sentiment classification', 'ences sentiment analysis', 'tweets deep learning', 'sentiment analysis aims', '2018 extraction sentiment', 'sentiment analysis entity', 'sentiment classification product', 'aspect based sentiment', 'sentiment analysis deep'] (6.69 dtk)\n",
            "üìù Ringkasan:  The increasing volume of user-generated content on the web has made sentiment analysis an important tool for the extraction of information about the ... (0.87 dtk)\n",
            "üìö Referensi diekstrak (68 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Akhtar, M. S., Gupta, D., Ekbal, A., & Bhattacharyya, P. (2017). Feature selection and to evolve to ...\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "[3/11]--- Processing: Explainable stock prices prediction from financial news articles using sentiment analysis.pdf ---\n",
            "üìÑ Teks diekstrak (48,888 chars) dalam 1.87 dtk.\n",
            "üîë Kata Kunci (10): ['term memory lstm', 'prediction accuracy lstm', 'price prediction hrituja', 'price prediction consistently', 'explainable stock prices', 'short term memory', 'stock price analysis', 'lstm explainable ai', 'use predict stock', 'stock data sentiments'] (5.25 dtk)\n",
            "üìù Ringkasan:  Explainable stock prices prediction from financial news articles using sentiment analysis to predict stock prices with a high level of precision . Au... (0.86 dtk)\n",
            "üìö Referensi diekstrak (1 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Pathmind, Inc. 2020. A beginner‚Äôs guide to LSTMs and recurrent neural networks. Available at https:/...\n",
            "üß† Embedding dibuat (dim: 384) (0.02 dtk)\n",
            "\n",
            "[4/11]--- Processing: Aspect-Based_Sentiment_Analysis_of_Twitter_Influencers_to_Predict_the_Trend_of_Cryptocurrencies_Based_on_Hybrid_Deep_Transfer_Learning_Models.pdf ---\n",
            "üìÑ Teks diekstrak (64,393 chars) dalam 7.04 dtk.\n",
            "üîë Kata Kunci (10): ['influencers tweets predict', 'transfer learning models', 'analysis twitter influencers', 'hot topics machine', 'oriented sentiment analysis', 'topics machine learning', 'different aspects tweets', 'predicting cryptocurrency text', 'sentiment process cryptocurrency', 'aspect oriented sentiment'] (5.09 dtk)\n",
            "üìù Ringkasan:  This paper presents a hybrid model of transfer-transfer-learning methods for the aspect-oriented sentiment analysis of influencers‚Äô tweets to predict... (1.85 dtk)\n",
            "üìö Referensi diekstrak (38 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: [24] J. Abraham, D. Higdon, J. Nelson, and J. Ibarra, ‚Äò‚ÄòCryptocurrency price prediction using tweet ...\n",
            "üß† Embedding dibuat (dim: 384) (0.02 dtk)\n",
            "\n",
            "[5/11]--- Processing: Forecasting directional bitcoin price returns using aspect-based sentiment analysis on online text data.pdf ---\n",
            "üìÑ Teks diekstrak (68,417 chars) dalam 3.28 dtk.\n",
            "üîë Kata Kunci (10): ['data sentiment', 'keywords cryptocurrency', 'textual data sentiment', 'joint topical sentiment', 'sentiment feature extraction', 'comparing topic sentiment', 'aspect based sentiment', 'forecasting directional bitcoin', 'cryptocurrency sentiment', 'keywords cryptocurrency sentiment'] (5.13 dtk)\n",
            "üìù Ringkasan:  Forecasting directional bitcoin price returns using aspect-based sentiment analysis on online text data . The emergence of cryptocurrency markets has... (0.62 dtk)\n",
            "üìö Referensi diekstrak (71 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Abraham, J., Higdon, D., Nelson, J., & Ibarra, J. (2018). Cryptocurrency price prediction using twee...\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "[6/11]--- Processing: S I LSTM  stock price prediction based on multiple data sources and sentiment analysis.pdf ---\n",
            "üìÑ Teks diekstrak (55,680 chars) dalam 5.60 dtk.\n",
            "üîë Kata Kunci (10): ['sentiment analysis shengting', 'technical indicators stock', 'data involve stock', 'data sources stock', 'sentiment called s_i_lstm', 'price prediction current', 'sentiment analysis connection', 'price prediction adopt', '1940101 s_i_lstm stock', 'ccos20 s_i_lstm stock'] (5.29 dtk)\n",
            "üìù Ringkasan:  Shengting Wu, Yuling Liu, Ziran Zou & Tien-Hsiung Weng (2022) S_I_LSTM: \"Stock price prediction based on multiple data sources and sentiment analysis... (0.95 dtk)\n",
            "üìö Referensi diekstrak (59 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Achkar, R., Elias-Sleiman, F., Ezzidine, H., & Haidar, N. (2018). Comparison of BPA-MLP and LSTM-RNN...\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "[7/11]--- Processing: An Entropy-based Evaluation for Sentiment Analysis of Stock Market Prices using Twitter Data.pdf ---\n",
            "üìÑ Teks diekstrak (31,028 chars) dalam 1.42 dtk.\n",
            "üîë Kata Kunci (10): ['analysis stock market', 'twitter distinguish market', 'information stock market', 'evaluation sentiment analysis', 'sentiment analysis derived', 'fact investors sentiment', 'keywords stock market', 'sentiment analysis insights', 'prediction stock', 'markets prediction considered'] (6.68 dtk)\n",
            "üìù Ringkasan:  An Entropy-based Evaluation for Sentiment Analysis of Stock Market Prices using Twitter Data . The paper explores whether estimations, in terms of th... (0.96 dtk)\n",
            "üìö Referensi diekstrak (16 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: [1] W. Antweiler and M. Z. Frank. Is all that talk just noise? the information [17] A. Mittal and A....\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "[8/11]--- Processing: Stock market prediction using machine learning classifiers and social media, news.pdf ---\n",
            "üìÑ Teks diekstrak (107,594 chars) dalam 5.33 dtk.\n",
            "üîë Kata Kunci (10): ['sentiment analysis stock', 'markets predicted', 'market prediction introduction', 'predictions feature', 'classifiers social media', 'financial news data', 'stock prediction stock', 'market prediction accuracy', 'stocks financial news', 'predict stock'] (6.71 dtk)\n",
            "üìù Ringkasan:  Stock markets can be predicted using machine learning algorithms on information contained in social media and financial news, as this data can change... (0.74 dtk)\n",
            "üìö Referensi diekstrak (1 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Springer, New York, p 745 Li J, Bu H, Wu J (2017) Sentiment-aware stock market prediction: He K, Zha...\n",
            "üß† Embedding dibuat (dim: 384) (0.02 dtk)\n",
            "\n",
            "[9/11]--- Processing: Harvesting social media sentiment analysis to enhance stock market prediction using deep learning.pdf ---\n",
            "üìÑ Teks diekstrak (49,306 chars) dalam 1.86 dtk.\n",
            "üîë Kata Kunci (10): ['use social network', 'stock price prediction', 'like facebook twitter', 'stock trend', 'blogs social', 'sentiment analysis machine', 'harvesting social media', 'sm machine learning', 'opinion stock market', 'keywords stock prediction'] (8.90 dtk)\n",
            "üìù Ringkasan:  The stock market (SM) is an essential area of the economy and plays a significant role in trade and industry development . Machine learning can provi... (0.78 dtk)\n",
            "‚Ñπ Heuristik pemisahan referensi gagal, mengembalikan sebagai blok teks.\n",
            "üìö Referensi diekstrak (1 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Alexander P, Ilya R, Alexey S. 2013. Machine learning in prediction of stock market indicators\n",
            "based...\n",
            "üß† Embedding dibuat (dim: 384) (0.02 dtk)\n",
            "\n",
            "[10/11]--- Processing: Trading on Twitter Using Social Media Sentiment to Predict Stock Returns.pdf ---\n",
            "üìÑ Teks diekstrak (91,578 chars) dalam 3.95 dtk.\n",
            "üîë Kata Kunci (10): ['media sentiment predict', 'twitter emotion', 'emotional sentiment', 'sentiment spreads slowly', 'expressed sentiment spreads', 'results sentiment tweets', 'firms results sentiment', 'social media sentiment', 'sentiment firm stock', 'sentiment predict stock'] (5.09 dtk)\n",
            "üìù Ringkasan:  Emotional sentiment about a firm‚Äôs stock that spreads rapidly through social media is more likely to be incorporated quickly into stock prices . Emot... (0.95 dtk)\n",
            "üìö Referensi diekstrak (77 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: Abbasi, A., & Chen, H. (2008). CyberGate: A design framework and system for text analysis of compute...\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "[11/11]--- Processing: Sentiment_Analysis_in_Financial_Markets_Using_NLP_to_Predict_Stock_Price_from_News_and_Social_Media_Data.pdf ---\n",
            "üìÑ Teks diekstrak (27,412 chars) dalam 1.24 dtk.\n",
            "üîë Kata Kunci (10): ['compare sentiment trends', 'mba sastra', 'results financial indicators', 'nlp predict stock', 'sastra deemed university', 'november sentiment data', 'deemed university sastra', 'management sastra', 'sentiment data sourced', 'analysis sentiment'] (6.35 dtk)\n",
            "üìù Ringkasan:  Using NLP to Predict Stock Price from News and Social Media Data, Authors of International Conference on Visual Analytics and Data Visualization (ICV... (0.75 dtk)\n",
            "üìö Referensi diekstrak (12 buah) (0.00 dtk)\n",
            " ¬† [Contoh Ref]: [1] Batra, R., & Daudpota, S. M. (2018, March). Integrating StockTwits with sentiment analysis for b...\n",
            "üß† Embedding dibuat (dim: 384) (0.01 dtk)\n",
            "\n",
            "==================================================\n",
            "‚úÖ Proses Ekstraksi & Analisis Inti Selesai!\n",
            "‚è± Total Waktu: 126.09 detik\n",
            "üìä Jumlah File Diproses: 11\n",
            "üìà Jumlah File dengan Embedding: 11\n",
            "\n",
            "üíæ Menyimpan hasil detail ke: /content/drive/MyDrive/PaperTA_results.json\n",
            "‚úÖ Hasil berhasil disimpan.\n"
          ]
        }
      ],
      "source": [
        "print(f\"üöÄ Memulai Proses Inti dari folder: {DATASET_FOLDER}\\n\" + \"=\"*50)\n",
        "\n",
        "results = {} # Stores keywords, summary, text length\n",
        "embeddings_map = {} # Stores filename -> embedding vector\n",
        "filenames_list = [] # Stores filenames in order of processing\n",
        "document_texts = {} # DICTIONARY BARU UNTUK MENYIMPAN TEKS LENGKAP\n",
        "\n",
        "try:\n",
        "    all_files = [f for f in os.listdir(DATASET_FOLDER) if f.lower().endswith(('.pdf', '.docx'))]\n",
        "    total_files = len(all_files)\n",
        "    if total_files == 0:\n",
        "        print(f\"‚ö† Tidak ada file PDF atau DOCX yang ditemukan di '{DATASET_FOLDER}'.\")\n",
        "    else:\n",
        "        print(f\"üîç Menemukan {total_files} file PDF/DOCX untuk diproses.\")\n",
        "except FileNotFoundError:\n",
        "    print(f\"‚ùå FATAL ERROR: Folder dataset tidak ditemukan di '{DATASET_FOLDER}'. Hentikan eksekusi.\")\n",
        "    all_files = []\n",
        "\n",
        "file_counter = 0\n",
        "start_time = time.time()\n",
        "\n",
        "# --- Main Loop ---\n",
        "for filename in all_files:\n",
        "    file_counter += 1\n",
        "    print(f\"\\n[{file_counter}/{total_files}]--- Processing: {filename} ---\")\n",
        "    file_path = os.path.join(DATASET_FOLDER, filename)\n",
        "\n",
        "    # Initialize result entry for this file\n",
        "    # TAMBAHKAN 'references': []\n",
        "    results[filename] = {'text_length': 0, 'keywords': [], 'summary': '', 'references': [], 'embedding': None}\n",
        "\n",
        "    # 1. Extract Text\n",
        "    full_text = None\n",
        "    start_extract = time.time()\n",
        "    if filename.lower().endswith(\".pdf\"):\n",
        "        full_text = extract_text_from_pdf(file_path)\n",
        "    elif filename.lower().endswith(\".docx\"):\n",
        "        full_text = extract_text_from_docx(file_path)\n",
        "    extract_duration = time.time() - start_extract\n",
        "\n",
        "    if full_text:\n",
        "        document_texts[filename] = full_text\n",
        "        text_len = len(full_text)\n",
        "        results[filename]['text_length'] = text_len\n",
        "        print(f\"üìÑ Teks diekstrak ({text_len:,} chars) dalam {extract_duration:.2f} dtk.\")\n",
        "\n",
        "        # 2. Extract Keywords\n",
        "        kw_start = time.time()\n",
        "        keywords = extract_keywords_bert(full_text, kw_model) # Fungsi sudah diperbaiki\n",
        "        results[filename]['keywords'] = keywords\n",
        "        kw_duration = time.time() - kw_start\n",
        "        print(f\"üîë Kata Kunci ({len(keywords)}): {keywords} ({kw_duration:.2f} dtk)\")\n",
        "\n",
        "        # 3. Generate Summary\n",
        "        sum_start = time.time()\n",
        "        summary_text = generate_summary_bart(full_text, summarizer) # Fungsi sudah diperbaiki\n",
        "        results[filename]['summary'] = summary_text\n",
        "        sum_duration = time.time() - sum_start\n",
        "        # Hanya tampilkan ringkasan jika tidak gagal\n",
        "        summary_display = summary_text[:150]+\"...\" if not summary_text.startswith(\"Gagal\") else summary_text\n",
        "        print(f\"üìù Ringkasan: {summary_display} ({sum_duration:.2f} dtk)\")\n",
        "\n",
        "        # 4. Extract References (FUNGSI BARU)\n",
        "        ref_start = time.time()\n",
        "        references = extract_references(full_text) # Panggil fungsi baru\n",
        "        results[filename]['references'] = references # Simpan hasilnya\n",
        "        ref_duration = time.time() - ref_start\n",
        "        print(f\"üìö Referensi diekstrak ({len(references)} buah) ({ref_duration:.2f} dtk)\")\n",
        "        # Tampilkan 1 referensi pertama sebagai sampel\n",
        "        if references:\n",
        "            print(f\" ¬† [Contoh Ref]: {references[0][:100]}...\")\n",
        "\n",
        "        # 5. Generate Embedding (sebelumnya #4)\n",
        "        emb_start = time.time()\n",
        "        embedding_vector = generate_embeddings(full_text, embedding_model) # Fungsi sudah diperbaiki\n",
        "        emb_duration = time.time() - emb_start\n",
        "        if embedding_vector is not None:\n",
        "            embeddings_map[filename] = embedding_vector\n",
        "            filenames_list.append(filename)\n",
        "            # Simpan sebagai list hanya jika berhasil\n",
        "            results[filename]['embedding'] = embedding_vector.tolist()\n",
        "            print(f\"üß† Embedding dibuat (dim: {embedding_vector.shape[0]}) ({emb_duration:.2f} dtk)\")\n",
        "        else:\n",
        "            # Pastikan embedding disimpan sebagai None jika gagal\n",
        "            results[filename]['embedding'] = None\n",
        "            print(f\"‚ùå Embedding Gagal.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"‚ùå Ekstraksi Teks Gagal.\")\n",
        "        results[filename]['error'] = 'Text extraction failed'\n",
        "        results[filename]['embedding'] = None # Pastikan None jika teks gagal\n",
        "\n",
        "\n",
        "# --- Finalization ---\n",
        "end_time = time.time()\n",
        "print(\"\\n\" + \"=\"*50 + f\"\\n‚úÖ Proses Ekstraksi & Analisis Inti Selesai!\")\n",
        "print(f\"‚è± Total Waktu: {end_time - start_time:.2f} detik\")\n",
        "print(f\"üìä Jumlah File Diproses: {file_counter}\")\n",
        "print(f\"üìà Jumlah File dengan Embedding: {len(embeddings_map)}\")\n",
        "\n",
        "# --- Save Results ---\n",
        "print(f\"\\nüíæ Menyimpan hasil detail ke: {RESULTS_JSON_PATH}\")\n",
        "try:\n",
        "    # Convert numpy arrays in embeddings_map to lists for JSON serialization\n",
        "    results_to_save = {}\n",
        "    for fname, data in results.items():\n",
        "        results_to_save[fname] = data.copy() # Avoid modifying original dict\n",
        "        if fname in embeddings_map:\n",
        "            # Check if embedding exists and is numpy array before converting\n",
        "            if embeddings_map[fname] is not None and isinstance(embeddings_map[fname], np.ndarray):\n",
        "                results_to_save[fname]['embedding'] = embeddings_map[fname].tolist()\n",
        "            else:\n",
        "                results_to_save[fname]['embedding'] = None # Store None if embedding failed\n",
        "        else:\n",
        "            results_to_save[fname]['embedding'] = None # Handle cases where embedding might not exist\n",
        "\n",
        "    with open(RESULTS_JSON_PATH, 'w') as f:\n",
        "        json.dump(results_to_save, f, indent=2)\n",
        "    print(f\"‚úÖ Hasil berhasil disimpan.\")\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Gagal menyimpan hasil ke JSON: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "o7oapMaYvHfw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d3622a26-5931-466e-b885-0dc2403bcc8c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üîó Memulai Analisis Hubungan Antar Dokumen...\n",
            "üìê Menghitung Cosine Similarity untuk 11 dokumen...\n",
            "‚úÖ Matriks Kemiripan ((11, 11)) selesai (0.00 dtk).\n",
            "\n",
            "--- Matriks Kemiripan (Nilai Sebenarnya) ---\n",
            "[[1.    0.562 0.671 0.531 0.658 0.502 0.705 0.639 0.617 0.589 0.526]\n",
            " [0.562 1.    0.521 0.679 0.638 0.324 0.501 0.519 0.539 0.435 0.357]\n",
            " [0.671 0.521 1.    0.443 0.54  0.606 0.664 0.697 0.715 0.503 0.495]\n",
            " [0.531 0.679 0.443 1.    0.701 0.3   0.417 0.503 0.51  0.339 0.286]\n",
            " [0.658 0.638 0.54  0.701 1.    0.458 0.591 0.614 0.589 0.516 0.464]\n",
            " [0.502 0.324 0.606 0.3   0.458 1.    0.523 0.539 0.508 0.514 0.558]\n",
            " [0.705 0.501 0.664 0.417 0.591 0.523 1.    0.729 0.708 0.707 0.532]\n",
            " [0.639 0.519 0.697 0.503 0.614 0.539 0.729 1.    0.784 0.628 0.495]\n",
            " [0.617 0.539 0.715 0.51  0.589 0.508 0.708 0.784 1.    0.654 0.566]\n",
            " [0.589 0.435 0.503 0.339 0.516 0.514 0.707 0.628 0.654 1.    0.572]\n",
            " [0.526 0.357 0.495 0.286 0.464 0.558 0.532 0.495 0.566 0.572 1.   ]]\n",
            "\n",
            "üìà Membangun data graf (Threshold > 0.5)...\n",
            "üìä Data Graf Siap: 11 nodes, 43 edges.\n",
            "\n",
            "üíæ Menyimpan data graf ke: /content/drive/MyDrive/PaperTA_graph_data.json\n",
            "‚úÖ Data graf berhasil disimpan.\n",
            "\n",
            "‚ú® Contoh 5 Hubungan Terkuat:\n",
            " g ¬†- Stock market prediction using ... <-> Harvesting social media sentim... (Sim: 0.784)\n",
            " g ¬†- An Entropy-based Evaluation fo... <-> Stock market prediction using ... (Sim: 0.729)\n",
            " g ¬†- Explainable stock prices predi... <-> Harvesting social media sentim... (Sim: 0.715)\n",
            " g ¬†- An Entropy-based Evaluation fo... <-> Harvesting social media sentim... (Sim: 0.708)\n",
            " g ¬†- An Entropy-based Evaluation fo... <-> Trading on Twitter Using Socia... (Sim: 0.707)\n",
            "\n",
            "==================================================\n",
            "‚úÖ Analisis Hubungan Selesai!\n"
          ]
        }
      ],
      "source": [
        "print(\"\\n\" + \"=\"*50 + \"\\nüîó Memulai Analisis Hubungan Antar Dokumen...\")\n",
        "\n",
        "# Prepare matrix only from successfully generated embeddings\n",
        "valid_embeddings = [embeddings_map[fname] for fname in filenames_list if embeddings_map.get(fname) is not None]\n",
        "valid_filenames = [fname for fname in filenames_list if embeddings_map.get(fname) is not None]\n",
        "\n",
        "if len(valid_embeddings) >= 2:\n",
        "    embedding_matrix = np.array(valid_embeddings)\n",
        "    print(f\"üìê Menghitung Cosine Similarity untuk {len(valid_filenames)} dokumen...\")\n",
        "    start_sim_time = time.time()\n",
        "    similarity_matrix = cosine_similarity(embedding_matrix)\n",
        "    end_sim_time = time.time()\n",
        "    print(f\"‚úÖ Matriks Kemiripan ({similarity_matrix.shape}) selesai ({end_sim_time - start_sim_time:.2f} dtk).\")\n",
        "    print(\"\\n--- Matriks Kemiripan (Nilai Sebenarnya) ---\")\n",
        "    print(similarity_matrix.round(3))\n",
        "\n",
        "    # --- Prepare Graph Data ---\n",
        "    nodes = []\n",
        "    edges = []\n",
        "    print(f\"\\nüìà Membangun data graf (Threshold > {SIMILARITY_THRESHOLD})...\")\n",
        "    for i in range(len(valid_filenames)):\n",
        "        fname_i = valid_filenames[i]\n",
        "        nodes.append({\"id\": fname_i, \"label\": fname_i[:25]+\"...\"}) # Slightly longer label\n",
        "\n",
        "        for j in range(i + 1, len(valid_filenames)):\n",
        "            fname_j = valid_filenames[j]\n",
        "            similarity_score = similarity_matrix[i][j]\n",
        "            if similarity_score > SIMILARITY_THRESHOLD:\n",
        "                edges.append({\n",
        "                    \"from\": fname_i, \"to\": fname_j,\n",
        "                    \"value\": float(similarity_score),\n",
        "                    \"title\": f\"Similarity: {similarity_score:.3f}\"\n",
        "                })\n",
        "\n",
        "    print(f\"üìä Data Graf Siap: {len(nodes)} nodes, {len(edges)} edges.\")\n",
        "\n",
        "    # --- Save Graph Data ---\n",
        "    graph_data = {\"nodes\": nodes, \"edges\": edges}\n",
        "    print(f\"\\nüíæ Menyimpan data graf ke: {GRAPH_JSON_PATH}\")\n",
        "    try:\n",
        "        with open(GRAPH_JSON_PATH, 'w') as f:\n",
        "            json.dump(graph_data, f, indent=2)\n",
        "        print(f\"‚úÖ Data graf berhasil disimpan.\")\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Gagal menyimpan data graf: {e}\")\n",
        "\n",
        "    # --- Show Strongest Relationships ---\n",
        "    if edges:\n",
        "        sorted_edges = sorted(edges, key=lambda x: x['value'], reverse=True)\n",
        "        print(\"\\n‚ú® Contoh 5 Hubungan Terkuat:\")\n",
        "        for edge in sorted_edges[:5]:\n",
        "            print(f\" g ¬†- {edge['from'][:30]}... <-> {edge['to'][:30]}... (Sim: {edge['value']:.3f})\")\n",
        "    else:\n",
        "        print(\"‚Ñπ Tidak ditemukan hubungan signifikan di atas threshold.\")\n",
        "\n",
        "else:\n",
        "    print(\"‚ö† Jumlah dokumen dengan embedding valid kurang dari 2. Analisis hubungan tidak dapat dilakukan.\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n‚úÖ Analisis Hubungan Selesai!\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.cluster import KMeans\n",
        "from collections import Counter\n",
        "import itertools\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\nüí° Memulai Inovasi 2: Identifikasi Celah Riset Terpersonalisasi...\")\n",
        "\n",
        "# --- Memeriksa Ketersediaan Data ---\n",
        "required_vars_inno2 = ['embeddings_map', 'filenames_list', 'results']\n",
        "vars_exist_inno2 = True\n",
        "for var in required_vars_inno2:\n",
        "    if var not in globals():\n",
        "        print(f\"‚ùå FATAL ERROR: Variabel '{var}' belum terdefinisi. Pastikan Cell 5 sudah dijalankan.\")\n",
        "        vars_exist_inno2 = False\n",
        "if not vars_exist_inno2:\n",
        "    raise NameError(\"Variabel penting untuk Inovasi 2 tidak ditemukan.\")\n",
        "\n",
        "# Filter hanya file yang berhasil di-embed\n",
        "valid_filenames_inno2 = [fname for fname in filenames_list if embeddings_map.get(fname) is not None]\n",
        "valid_embeddings_inno2 = [embeddings_map[fname] for fname in valid_filenames_inno2 if embeddings_map.get(fname) is not None]\n",
        "\n",
        "if len(valid_embeddings_inno2) < 3: # Butuh setidaknya 3 dokumen untuk clustering yang berarti\n",
        "    print(f\"‚ö† Jumlah dokumen dengan embedding valid ({len(valid_embeddings_inno2)}) terlalu sedikit untuk analisis kluster yang bermakna.\")\n",
        "else:\n",
        "    print(f\"‚úÖ Menemukan {len(valid_embeddings_inno2)} dokumen dengan embedding valid untuk dianalisis klusternya.\")\n",
        "    embedding_matrix_inno2 = np.array(valid_embeddings_inno2)\n",
        "\n",
        "    # --- Langkah 1: Klasterisasi Dokumen (K-Means) ---\n",
        "    # Menentukan jumlah kluster (heuristic sederhana)\n",
        "    # Target: sekitar 2-5 kluster, tapi tidak lebih dari jumlah dokumen / 2\n",
        "    num_documents = embedding_matrix_inno2.shape[0]\n",
        "    n_clusters = max(2, min(5, num_documents // 2))\n",
        "    print(f\"\\n‚è≥ Melakukan klasterisasi K-Means dengan k={n_clusters}...\")\n",
        "    start_cluster_time = time.time()\n",
        "    try:\n",
        "        kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # n_init='auto' atau 10\n",
        "        cluster_labels = kmeans.fit_predict(embedding_matrix_inno2)\n",
        "        end_cluster_time = time.time()\n",
        "        print(f\"‚úÖ Klasterisasi selesai ({end_cluster_time - start_cluster_time:.2f} dtk).\")\n",
        "\n",
        "        # --- Langkah 2: Analisis Konten Kluster (Menggunakan Kata Kunci) ---\n",
        "        print(\"\\nüìä Menganalisis konten kluster berdasarkan kata kunci...\")\n",
        "        clusters = {i: [] for i in range(n_clusters)} # {cluster_id: [filename1, filename2,...]}\n",
        "        cluster_keywords = {i: Counter() for i in range(n_clusters)} # {cluster_id: Counter(keyword: count)}\n",
        "\n",
        "        for filename, label in zip(valid_filenames_inno2, cluster_labels):\n",
        "            clusters[label].append(filename)\n",
        "            # Ambil kata kunci dari hasil Cell 5\n",
        "            if filename in results and 'keywords' in results[filename]:\n",
        "                # Update counter kata kunci untuk kluster ini\n",
        "                cluster_keywords[label].update(results[filename]['keywords'])\n",
        "\n",
        "        # Tampilkan hasil klasterisasi dan kata kunci dominan\n",
        "        print(\"\\n--- Hasil Klasterisasi Dokumen ---\")\n",
        "        for i in range(n_clusters):\n",
        "            print(f\"\\n ¬†Cluster {i}:\")\n",
        "            print(f\" ¬† ¬†Dokumen ({len(clusters[i])}):\")\n",
        "            for fname in clusters[i]:\n",
        "                print(f\" ¬† ¬† ¬†- {fname}\")\n",
        "            # Tampilkan 5 kata kunci paling umum di kluster ini\n",
        "            top_5_keywords = cluster_keywords[i].most_common(5)\n",
        "            print(f\" ¬† ¬†Top Keywords: {[kw for kw, count in top_5_keywords]}\")\n",
        "\n",
        "        # --- Langkah 3: Identifikasi Potensi Celah Riset (Heuristik Sederhana) ---\n",
        "        print(\"\\n\\nüí° Mencari Potensi Celah Riset (Area Persimpangan)...\")\n",
        "        suggested_gaps = []\n",
        "        # Cari kombinasi 2 kluster yang berbeda\n",
        "        for i, j in itertools.combinations(range(n_clusters), 2):\n",
        "            # Ambil 3 keyword teratas dari masing-masing kluster\n",
        "            keywords_i = [kw for kw, count in cluster_keywords[i].most_common(3)]\n",
        "            keywords_j = [kw for kw, count in cluster_keywords[j].most_common(3)]\n",
        "\n",
        "            # Buat saran sederhana dengan menggabungkan keyword\n",
        "            if keywords_i and keywords_j: # Pastikan ada keyword\n",
        "                # Saran 1: Keyword teratas dari i + Keyword teratas dari j\n",
        "                suggestion1 = f\"Kombinasi antara '{keywords_i[0]}' (dari Cluster {i}) dan '{keywords_j[0]}' (dari Cluster {j})\"\n",
        "                suggested_gaps.append(suggestion1)\n",
        "                # Saran 2 (jika keyword cukup): Keyword kedua dari i + Keyword kedua dari j\n",
        "                if len(keywords_i) > 1 and len(keywords_j) > 1:\n",
        "                    suggestion2 = f\"Persimpangan area '{keywords_i[1]}' (Cluster {i}) dan '{keywords_j[1]}' (Cluster {j})\"\n",
        "                    suggested_gaps.append(suggestion2)\n",
        "\n",
        "        if suggested_gaps:\n",
        "            print(\"\\n--- Saran Area Potensial (Berdasarkan Koleksi Anda Saat Ini) ---\")\n",
        "            for idx, gap in enumerate(suggested_gaps[:5]): # Tampilkan maksimal 5 saran\n",
        "                print(f\" ¬†{idx+1}. {gap}\")\n",
        "            print(\"\\n(Saran ini bersifat heuristik, perlu validasi lebih lanjut)\")\n",
        "        else:\n",
        "            print(\"‚Ñπ Tidak dapat menghasilkan saran celah riset otomatis dari kluster yang ada.\")\n",
        "\n",
        "    except Exception as cluster_error:\n",
        "        print(f\"‚ùå Error saat melakukan klasterisasi atau analisis: {cluster_error}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*50 + \"\\n‚úÖ Inovasi 2 Selesai!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fM0i9fkAdM9U",
        "outputId": "2d7747af-4e1e-4fa9-bf3a-2c5dec2c399e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "üí° Memulai Inovasi 2: Identifikasi Celah Riset Terpersonalisasi...\n",
            "‚úÖ Menemukan 11 dokumen dengan embedding valid untuk dianalisis klusternya.\n",
            "\n",
            "‚è≥ Melakukan klasterisasi K-Means dengan k=5...\n",
            "‚úÖ Klasterisasi selesai (0.03 dtk).\n",
            "\n",
            "üìä Menganalisis konten kluster berdasarkan kata kunci...\n",
            "\n",
            "--- Hasil Klasterisasi Dokumen ---\n",
            "\n",
            " ¬†Cluster 0:\n",
            " ¬† ¬†Dokumen (3):\n",
            " ¬† ¬† ¬†- Deep Learning for Aspect-Based Sentiment Analysis A Comparative Review.pdf\n",
            " ¬† ¬† ¬†- Aspect-Based_Sentiment_Analysis_of_Twitter_Influencers_to_Predict_the_Trend_of_Cryptocurrencies_Based_on_Hybrid_Deep_Transfer_Learning_Models.pdf\n",
            " ¬† ¬† ¬†- Forecasting directional bitcoin price returns using aspect-based sentiment analysis on online text data.pdf\n",
            " ¬† ¬†Top Keywords: ['aspect based sentiment', 'content web sentiment', 'reviews sentiment classification', 'ences sentiment analysis', 'tweets deep learning']\n",
            "\n",
            " ¬†Cluster 1:\n",
            " ¬† ¬†Dokumen (3):\n",
            " ¬† ¬† ¬†- Using Financial News Sentiment for Stock Price Direction Prediction.pdf\n",
            " ¬† ¬† ¬†- An Entropy-based Evaluation for Sentiment Analysis of Stock Market Prices using Twitter Data.pdf\n",
            " ¬† ¬† ¬†- Trading on Twitter Using Social Media Sentiment to Predict Stock Returns.pdf\n",
            " ¬† ¬†Top Keywords: ['sentiment information analysis', 'prediction keywords sentiment', 'sentiment scores titles', 'predicted sentiment scores', 'sentiment information texts']\n",
            "\n",
            " ¬†Cluster 2:\n",
            " ¬† ¬†Dokumen (1):\n",
            " ¬† ¬† ¬†- Sentiment_Analysis_in_Financial_Markets_Using_NLP_to_Predict_Stock_Price_from_News_and_Social_Media_Data.pdf\n",
            " ¬† ¬†Top Keywords: ['compare sentiment trends', 'mba sastra', 'results financial indicators', 'nlp predict stock', 'sastra deemed university']\n",
            "\n",
            " ¬†Cluster 3:\n",
            " ¬† ¬†Dokumen (1):\n",
            " ¬† ¬† ¬†- S I LSTM  stock price prediction based on multiple data sources and sentiment analysis.pdf\n",
            " ¬† ¬†Top Keywords: ['sentiment analysis shengting', 'technical indicators stock', 'data involve stock', 'data sources stock', 'sentiment called s_i_lstm']\n",
            "\n",
            " ¬†Cluster 4:\n",
            " ¬† ¬†Dokumen (3):\n",
            " ¬† ¬† ¬†- Explainable stock prices prediction from financial news articles using sentiment analysis.pdf\n",
            " ¬† ¬† ¬†- Stock market prediction using machine learning classifiers and social media, news.pdf\n",
            " ¬† ¬† ¬†- Harvesting social media sentiment analysis to enhance stock market prediction using deep learning.pdf\n",
            " ¬† ¬†Top Keywords: ['term memory lstm', 'prediction accuracy lstm', 'price prediction hrituja', 'price prediction consistently', 'explainable stock prices']\n",
            "\n",
            "\n",
            "üí° Mencari Potensi Celah Riset (Area Persimpangan)...\n",
            "\n",
            "--- Saran Area Potensial (Berdasarkan Koleksi Anda Saat Ini) ---\n",
            " ¬†1. Kombinasi antara 'aspect based sentiment' (dari Cluster 0) dan 'sentiment information analysis' (dari Cluster 1)\n",
            " ¬†2. Persimpangan area 'content web sentiment' (Cluster 0) dan 'prediction keywords sentiment' (Cluster 1)\n",
            " ¬†3. Kombinasi antara 'aspect based sentiment' (dari Cluster 0) dan 'compare sentiment trends' (dari Cluster 2)\n",
            " ¬†4. Persimpangan area 'content web sentiment' (Cluster 0) dan 'mba sastra' (Cluster 2)\n",
            " ¬†5. Kombinasi antara 'aspect based sentiment' (dari Cluster 0) dan 'sentiment analysis shengting' (dari Cluster 3)\n",
            "\n",
            "(Saran ini bersifat heuristik, perlu validasi lebih lanjut)\n",
            "\n",
            "==================================================\n",
            "‚úÖ Inovasi 2 Selesai!\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}